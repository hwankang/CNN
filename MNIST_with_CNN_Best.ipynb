{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "MNIST with CNN Best.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KelPyW3z5_gX",
        "33t0XKtr5_gg",
        "F3_90zbS5_gl",
        "qx6C5yOh5_gq",
        "JvUfSaSl5_gw"
      ],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwankang/CNN/blob/main/MNIST_with_CNN_Best.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDKhUzHd5_fV"
      },
      "source": [
        "# Best CNN architecture    \n",
        "* In this kernel, we will run experiments \n",
        "* to find the most accurate and efficient CNN architecture \n",
        "* for classifying MNIST handwritten digits.\n",
        "---\n",
        "* Author: Digit Recognizer, Kaggle (2020.6)\n",
        "* Lecture: Jason Dong (2020.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvLLGYql5_fY"
      },
      "source": [
        "# 1. 라이브러리 임포트, 데이터 로딩, 데이터 탐색"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEpX_vmx5_fZ"
      },
      "source": [
        "Here I'm using mnist original dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omVpXj5v73xA"
      },
      "source": [
        "# 라이브러리 임포트\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Plot ad hoc mnist instances\n",
        "from keras.datasets import mnist\n",
        "import ###.pyplot as plt\n",
        "\n",
        "print(np.__version__)\n",
        "print(pd.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSO2azLZ77Mb"
      },
      "source": [
        "# 데이터 로딩\n",
        "# load (downloaded if needed) the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.###()\n",
        "\n",
        "print(X_train[:1])\n",
        "#print(y_train[:1])\n",
        "#print(X_test[:1])\n",
        "#print(y_test[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "5AoaaYQH5_fu"
      },
      "source": [
        "# 이미지 데이터 시각화 및 탐색\n",
        "# plot 4 images as gray scale\n",
        "plt.###(221)\n",
        "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(###)\n",
        "plt.###(X_train[1], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(223)\n",
        "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(###)\n",
        "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
        "\n",
        "# show the plot\n",
        "plt.###()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su4d0Pts5_gC"
      },
      "source": [
        "# 2. MNIST 손글씨 픽셀 데이터의 시각화 / visualization\n",
        "\n",
        "* 28x28\n",
        "* 784px\n",
        "* 각 픽셀마다 색상은 백 0 ~ 흑 255\n",
        "* 각 픽셀마다 색상을 흑 0 ~ 백 1으로 정규화 필요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2o1C_Xu5_gD"
      },
      "source": [
        "#visualize_input()\n",
        "#imshow()\n",
        "#add_subplot()\n",
        "\n",
        "def ###(img, ax):\n",
        "    ax.###(img, cmap='gray')\n",
        "    width, height = img.shape\n",
        "    thresh = img.max()/2.5\n",
        "    for x in range(width):\n",
        "        for y in range(height):\n",
        "            ax.annotate(str(round(img[x][y],2)), xy=(y,x),\n",
        "                        horizontalalignment='center',\n",
        "                        verticalalignment='center',\n",
        "                        color='white' if img[x][y]<thresh else 'black')\n",
        "\n",
        "fig = plt.figure(figsize = (12,12)) \n",
        "ax = fig.###(111)\n",
        "visualize_input(X_train[1].reshape(28,28), ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZPSrAxm5_gJ"
      },
      "source": [
        "import seaborn as sns\n",
        "# countplot()\n",
        "\n",
        "g = sns.###(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALNi8vlp5_gR"
      },
      "source": [
        "# Different layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpLkJkO05_gS"
      },
      "source": [
        "The CNN can isolate features that are useful everywhere from these transformed images (feature maps)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQyN8kDF5_gS"
      },
      "source": [
        "# 1. **convolutional (Conv2D) layer**:    \n",
        "It is like a set of learnable filters. Each filter transforms a part of the image (defined by the kernel size) using the kernel filter. The kernel filter matrix is applied on the whole image. Filters can be seen as a transformation of the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6HnVp185_gT"
      },
      "source": [
        "![](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/35_blog_image_12.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHn2pkK35_gU"
      },
      "source": [
        "## Conv2D class: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llSgv4zP5_gU"
      },
      "source": [
        "![](https://raw.githubusercontent.com/soham1024/learn-cnn/master/Screenshot%20from%202020-05-09%2012-34-21.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVlckKyh5_gV"
      },
      "source": [
        "This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well.                     \n",
        "                         \n",
        "When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEqVXZer5_gW"
      },
      "source": [
        "![](https://raw.githubusercontent.com/soham1024/learn-cnn/master/Screenshot%20from%202020-05-09%2013-12-22.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KelPyW3z5_gX"
      },
      "source": [
        "### Arguments:                     \n",
        "\n",
        "*     filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
        "*     kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n",
        "*     strides: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1.\n",
        "*     padding: one of \"valid\" or \"same\" (case-insensitive).\n",
        "*     data_format: A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch_size, height, width, channels) while channels_first corresponds to inputs with shape (batch_size, channels, height, width). It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json. If you never set it, then it will be \"channels_last\".\n",
        "*     dilation_rate: an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1.\n",
        "*     activation: Activation function to use. If you don't specify anything, no activation is applied ( see keras.activations).\n",
        "*    use_bias: Boolean, whether the layer uses a bias vector.\n",
        "*    kernel_initializer: Initializer for the kernel weights matrix ( see keras.initializers).\n",
        "*    bias_initializer: Initializer for the bias vector ( see keras.initializers).\n",
        "*    kernel_regularizer: Regularizer function applied to the kernel weights matrix (see keras.regularizers).\n",
        "*    bias_regularizer: Regularizer function applied to the bias vector ( see keras.regularizers).\n",
        "*    activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\") ( see keras.regularizers).\n",
        "*    kernel_constraint: Constraint function applied to the kernel matrix ( see keras.constraints).\n",
        "*    bias_constraint: Constraint function applied to the bias vector ( see keras.constraints).\n",
        "\n",
        "**Input shape**\n",
        "\n",
        "*    4D tensor with shape: (batch_size, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (batch_size, rows, cols, channels) if data_format='channels_last'.\n",
        "\n",
        "**Output shape**\n",
        "\n",
        "*    4D tensor with shape: (batch_size, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (batch_size, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding.\n",
        "\n",
        "**Returns**\n",
        "\n",
        "*    A tensor of rank 4 representing activation(conv2d(inputs, kernel) + bias).\n",
        "\n",
        "**Raises**\n",
        "\n",
        "*    ValueError: if padding is \"causal\".\n",
        "*    ValueError: when both strides > 1 and dilation_rate > 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eWcwVyq5_gY"
      },
      "source": [
        "# 2. **pooling (MaxPool2D) layer**:         \n",
        "This layer simply acts as a downsampling filter. It looks at the 2 neighboring pixels and picks the maximal value. These are used to reduce computational cost, and to some extent also reduce overfitting. We have to choose the pooling size (i.e the area size pooled each time) more the pooling dimension is high, more the downsampling is important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLJT6F255_gY"
      },
      "source": [
        "![](https://qph.fs.quoracdn.net/main-qimg-40cdeb3b43594f4b1b1b6e2c137e80b7.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31AgbU1H5_gZ"
      },
      "source": [
        "Combining convolutional and pooling layers, CNN are able to combine local features and learn more global features of the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkEvkCk55_ga"
      },
      "source": [
        "![](https://raw.githubusercontent.com/soham1024/learn-cnn/master/Screenshot%20from%202020-05-09%2013-07-34.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kic0eNzc5_gb"
      },
      "source": [
        "Max pooling operation for 2D spatial data.                 \n",
        "                        \n",
        "Downsamples the input representation by taking the maximum value over the window defined by pool_size for each dimension along the features axis. The window is shifted by strides in each dimension. The resulting output when using \"valid\" padding option has a shape(number of rows or columns) of: output_shape = (input_shape - pool_size + 1) / strides)                        \n",
        "                                   \n",
        "The resulting output shape when using the \"same\" padding option is: output_shape = input_shape / strides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ9LSTRE5_gc"
      },
      "source": [
        "![](https://raw.githubusercontent.com/soham1024/learn-cnn/master/Screenshot%20from%202020-05-09%2013-08-21.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPtl765p5_gd"
      },
      "source": [
        "![](https://raw.githubusercontent.com/soham1024/learn-cnn/master/Screenshot%20from%202020-05-09%2014-30-54.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLbQ9K5B5_gd"
      },
      "source": [
        "![](https://raw.githubusercontent.com/soham1024/learn-cnn/master/Screenshot%20from%202020-05-09%2014-32-24.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TuLuEOX5_gf"
      },
      "source": [
        "![](https://raw.githubusercontent.com/soham1024/learn-cnn/master/Screenshot%20from%202020-05-09%2014-34-12.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33t0XKtr5_gg"
      },
      "source": [
        "### Arguments\n",
        "\n",
        "*    pool_size: integer or tuple of 2 integers, window size over which to take the maximum. (2, 2) will take the max value over a 2x2 pooling window. If only one integer is specified, the same window length will be used for both dimensions.\n",
        "*    strides: Integer, tuple of 2 integers, or None. Strides values. Specifies how far the pooling window moves for each pooling step. If None, it will default to pool_size.\n",
        "*    padding: One of \"valid\" or \"same\" (case-insensitive). \"valid\" adds no zero padding. \"same\" adds padding such that if the stride is 1, the output shape is the same as input shape.\n",
        "*    data_format: A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width). It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json. If you never set it, then it will be \"channels_last\".\n",
        "\n",
        "**Input shape**\n",
        "\n",
        "*    If data_format='channels_last': 4D tensor with shape (batch_size, rows, cols, channels).\n",
        "*    If data_format='channels_first': 4D tensor with shape (batch_size, channels, rows, cols).\n",
        "\n",
        "**Output shape**\n",
        "\n",
        "*    If data_format='channels_last': 4D tensor with shape (batch_size, pooled_rows, pooled_cols, channels).\n",
        "*    If data_format='channels_first': 4D tensor with shape (batch_size, channels, pooled_rows, pooled_cols).\n",
        "\n",
        "**Returns**\n",
        "\n",
        "*    A tensor of rank 4 representing the maximum pooled values. See above for output shape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "811EBqJB5_gh"
      },
      "source": [
        "# 3. **Dropout**:      \n",
        "It is a regularization method, where a proportion of nodes in the layer are randomly ignored (setting their wieghts to zero) for each training sample. This drops randomly a propotion of the network and forces the network to learn features in a distributed way. This technique also improves generalization and reduces the overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYDszqV55_gh"
      },
      "source": [
        "![](https://www.oreilly.com/library/view/deep-learning-for/9781788295628/assets/d4d20bd7-192c-48e7-9da2-6d3ddc7929e7.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xViHER4p5_gi"
      },
      "source": [
        "![](https://raw.githubusercontent.com/soham1024/learn-cnn/master/Screenshot%20from%202020-05-09%2014-39-46.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BSKV-k25_gj"
      },
      "source": [
        "The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.\n",
        "\n",
        "Note that the Dropout layer only applies when training is set to True such that no values are dropped during inference. When using model.fit, training will be appropriately set to True automatically, and in other contexts, you can set the kwarg explicitly to True when calling the layer.\n",
        "\n",
        "(This is in contrast to setting trainable=False for a Dropout layer. trainable does not affect the layer's behavior, as Dropout does not have any variables/weights that can be frozen during training.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0j6O-R25_gk"
      },
      "source": [
        "![](https://raw.githubusercontent.com/soham1024/learn-cnn/master/Screenshot%20from%202020-05-09%2014-58-44.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3_90zbS5_gl"
      },
      "source": [
        "### Arguments:\n",
        "\n",
        "*    rate: Float between 0 and 1. Fraction of the input units to drop.\n",
        "*    noise_shape: 1D integer tensor representing the shape of the binary dropout mask that will be multiplied with the input. For instance, if your inputs have shape (batch_size, timesteps, features) and you want the dropout mask to be the same for all timesteps, you can use noise_shape=(batch_size, 1, features).\n",
        " *   seed: A Python integer to use as random seed.\n",
        "\n",
        "**Call arguments:**\n",
        "\n",
        "    inputs: Input tensor (of any rank).\n",
        "    training: Python boolean indicating whether the layer should behave in training mode (adding dropout) or in inference mode (doing nothing).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hgk1vOWc5_gl"
      },
      "source": [
        "# **relu**:     \n",
        "It is the rectifier (activation function max(0,x). The rectifier activation function is used to add non linearity to the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAZfDwHG5_gm"
      },
      "source": [
        "![](https://qph.fs.quoracdn.net/main-qimg-07bc0ec05532caf5ebe8b4c82d0f5ca3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXzMqyHv5_gn"
      },
      "source": [
        "![](https://i.pinimg.com/originals/bd/31/d2/bd31d2c58e90916640168e31014595cf.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AB803GA5_go"
      },
      "source": [
        "![](https://raw.githubusercontent.com/soham1024/learn-cnn/master/Screenshot%20from%202020-05-09%2015-15-06.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVZODKEk5_gp"
      },
      "source": [
        "Rectified Linear Unit activation function.\n",
        "\n",
        "With default values, it returns element-wise max(x, 0).\n",
        "\n",
        "Otherwise, it follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0X1hc2c5_gp"
      },
      "source": [
        "![](https://raw.githubusercontent.com/soham1024/learn-cnn/master/Screenshot%20from%202020-05-09%2015-17-34.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx6C5yOh5_gq"
      },
      "source": [
        "### Arguments :\n",
        "\n",
        "*    max_value: Float >= 0. Maximum activation value. Default to None, which means unlimited.\n",
        "*    negative_slope: Float >= 0. Negative slope coefficient. Default to 0.\n",
        "*    threshold: Float. Threshold value for thresholded activation. Default to 0.\n",
        "\n",
        "**Input shape**\n",
        "\n",
        "*    Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the batch axis) when using this layer as the first layer in a model.\n",
        "\n",
        "**Output shape**\n",
        "\n",
        "*    Same shape as the input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cHs9Oq-5_gr"
      },
      "source": [
        "# 4. Flatten layer:    \n",
        "It is use to convert the final feature maps into a one single 1D vector. This flattening step is needed so that you can make use of fully connected layers after some convolutional/maxpool layers. It combines all the found local features of the previous convolutional layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zSHFiX65_gs"
      },
      "source": [
        "![](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/73_blog_image_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0DX4jwf5_gt"
      },
      "source": [
        "Flattens the input. Does not affect the batch size.\n",
        "\n",
        "Note: If inputs are shaped (batch,) without a feature axis, then flattening adds an extra channel dimension and output shape is (batch, 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwqdCkBP5_gt"
      },
      "source": [
        "![](https://raw.githubusercontent.com/soham1024/learn-cnn/master/Screenshot%20from%202020-05-09%2015-22-21.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP9seS0I5_gu"
      },
      "source": [
        "![](https://raw.githubusercontent.com/soham1024/learn-cnn/master/Screenshot%20from%202020-05-09%2015-24-51.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvUfSaSl5_gw"
      },
      "source": [
        "### Arguments:\n",
        "\n",
        "*    data_format: A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, ..., channels) while channels_first corresponds to inputs with shape (batch, channels, ...). It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json. If you never set it, then it will be \"channels_last\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_33pmd-U5_gx"
      },
      "source": [
        "# 5. Dense layer:        \n",
        "It is just artificial neural networks (ANN) classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfRKJ0se5_gy"
      },
      "source": [
        "![](https://deeplizard.com/images/deep%20neural%20network%20with%204%20layers.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVSuJGoI5_gz"
      },
      "source": [
        "![](https://raw.githubusercontent.com/soham1024/learn-cnn/master/Screenshot%20from%202020-05-09%2015-27-03.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AidYFPaS5_gz"
      },
      "source": [
        "Just your regular densely-connected NN layer.\n",
        "\n",
        "Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True).\n",
        "\n",
        "Note: If the input to the layer has a rank greater than 2, then Dense computes the dot product between the inputs and the kernel along the last axis of the inputs and axis 1 of the kernel (using tf.tensordot). For example, if input has dimensions (batch_size, d0, d1), then we create a kernel with shape (d1, units), and the kernel operates along axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there are batch_size * d0 such sub-tensors). The output in this case will have shape (batch_size, d0, units).\n",
        "\n",
        "Besides, layer attributes cannot be modified after the layer has been called once (except the trainable attribute)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6Q8lm235_g0"
      },
      "source": [
        "![](https://raw.githubusercontent.com/soham1024/learn-cnn/master/Screenshot%20from%202020-05-09%2015-29-12.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RauoGJDk5_g1"
      },
      "source": [
        "### Arguments: \n",
        "\n",
        "*    units: Positive integer, dimensionality of the output space.\n",
        "*    activation: Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
        "*    use_bias: Boolean, whether the layer uses a bias vector.\n",
        "*    kernel_initializer: Initializer for the kernel weights matrix.\n",
        "*    bias_initializer: Initializer for the bias vector.\n",
        "*    kernel_regularizer: Regularizer function applied to the kernel weights matrix.\n",
        "*    bias_regularizer: Regularizer function applied to the bias vector.\n",
        "*    activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\").\n",
        "*    kernel_constraint: Constraint function applied to the kernel weights matrix.\n",
        "*    bias_constraint: Constraint function applied to the bias vector.\n",
        "\n",
        "**Input shape**\n",
        "\n",
        "*    N-D tensor with shape: (batch_size, ..., input_dim). The most common situation would be a 2D input with shape (batch_size, input_dim).\n",
        "\n",
        "**Output shape**\n",
        "\n",
        "*    N-D tensor with shape: (batch_size, ..., units). For instance, for a 2D input with shape (batch_size, input_dim), the output would have shape (batch_size, units)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyEZxaSH5_g1"
      },
      "source": [
        "####################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUpaK3Hd5_g2"
      },
      "source": [
        "**I've used the Keras Sequential API, where you just have to add one layer at a time, starting from the input.**\n",
        "\n",
        "In the last layer(Dense(X,activation=\"softmax\")) the net outputs distribution of probability of each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXIzMvuy5_g2"
      },
      "source": [
        "![](https://miro.medium.com/max/3600/1*dOv2a1ctNrHDo8Zks30Bbw.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMiDBIgj5_g3"
      },
      "source": [
        "from keras.### import mnist\n",
        "from keras.### import Sequential\n",
        "from keras.### import Dense\n",
        "from keras.### import Dropout\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWbqbm8b5_g6"
      },
      "source": [
        "#load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.###()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3TY2KAi5_g-"
      },
      "source": [
        "# Flatten"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pioLNiLc5_g-"
      },
      "source": [
        "#flatten 28*28 images to a 784 vector for each image\n",
        "#reshape()\n",
        "\n",
        "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
        "X_train = X_train.###((X_train.shape[0], num_pixels)).astype('float32')\n",
        "X_test = X_test.###((X_test.shape[0], num_pixels)).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6qUWLBR5_hB"
      },
      "source": [
        "# Normalize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jczhIqFW5_hC"
      },
      "source": [
        "# normalize inputs from 0-255 to 0-1\n",
        "\n",
        "X_train = X_train / ###\n",
        "X_test = X_test / ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_2sm-y75_hG"
      },
      "source": [
        "# One Hot Encode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO1UCfjw5_hG"
      },
      "source": [
        "# one hot encode outputs\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emu86EQU5_hJ"
      },
      "source": [
        "# Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcM95Do55_hK"
      },
      "source": [
        "# define baseline model\n",
        "def baseline_model():\n",
        "\t# create model\n",
        "\tmodel = ###()\n",
        "\tmodel.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
        "\t# Compile model\n",
        "\tmodel.###(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsuNeA3x5_hM"
      },
      "source": [
        "# build the model\n",
        "# summary()\n",
        "\n",
        "model = baseline_model()\n",
        "model.###()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LBnBUMA5_hP"
      },
      "source": [
        "# Fit the model\n",
        "model.###(###, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model.###(X_test, ###, verbose=0)\n",
        "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INMCx_c_5_hS"
      },
      "source": [
        "# Simple CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KfnX6Yw5_hT"
      },
      "source": [
        "# Simple CNN for the MNIST Dataset\n",
        "from keras.datasets import ###\n",
        "from keras.models import ###\n",
        "from keras.layers import Dense\n",
        "from keras.### import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# CNN layers: convolutional\n",
        "from keras.layers.### import Conv2D\n",
        "from keras.layers.### import MaxPooling2D\n",
        "\n",
        "# load data\n",
        "(X_train, ###), (###, y_test) = mnist.load_data()\n",
        "\n",
        "# reshape to be [samples][width][height][channels]\n",
        "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
        "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32')\n",
        "\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "# define a simple CNN model\n",
        "def baseline_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (5, 5), input_shape=(##, ##, 1), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D())\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu'))\n",
        "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(###='categorical_crossentropy', ###='adam', ###=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# build the model\n",
        "model_simple = baseline_model()\n",
        "model_simple.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo3cSvjr5_hX"
      },
      "source": [
        "# Fit the model\n",
        "model_simple.fit(###, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model_simple.evaluate(X_test, ###, verbose=1)\n",
        "\n",
        "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dutd45ng5_ha"
      },
      "source": [
        "# Large CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EID0tp-v5_hc"
      },
      "source": [
        "# Large CNN for the MNIST Dataset\n",
        "from keras.datasets import ###\n",
        "from keras.models import Sequential\n",
        "from keras.### import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.### import Conv2D\n",
        "from keras.layers.convolutional import ###\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.###()\n",
        "\n",
        "# reshape to be [samples][width][height][channels]\n",
        "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
        "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32')\n",
        "\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "# define the larger model\n",
        "def large_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D())\n",
        "\tmodel.add(###(15, (3, 3), activation='relu'))\n",
        "\tmodel.add(###())\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu'))\n",
        "\tmodel.add(Dense(50, activation='relu'))\n",
        "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# build the model\n",
        "model_large = large_model()\n",
        "model_large.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVbRmCtt5_hh"
      },
      "source": [
        "# Fit the model\n",
        "model_large.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
        "# Final evaluation of the model\n",
        "\n",
        "scores = model_large.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxkF7E4M5_hk"
      },
      "source": [
        "# Larger CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C1m8r0o5_hk"
      },
      "source": [
        "# Larger CNN for the MNIST Dataset\n",
        "# Library import\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "\n",
        "# load data\n",
        "#\n",
        "\n",
        "# reshape to be [samples][width][height][channels]\n",
        "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
        "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32')\n",
        "\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = ###\n",
        "X_test = ###\n",
        "\n",
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "# define the larger model\n",
        "def larger_model():\n",
        "    # create model\n",
        "    model = ###()\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', input_shape=(##,##,1)))\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=2))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(Conv2D(filters=192, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=2))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Conv2D(filters=192, kernel_size=5, padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=2, padding='same'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(##, activation='softmax'))\n",
        "    # Compile model\n",
        "    model.###(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# build the model\n",
        "# model summary\n",
        "model_larger = larger_model()\n",
        "model_larger.###()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDM1zR1Q5_hn"
      },
      "source": [
        "# Fit the model\n",
        "model_larger.###(###, ###, validation_data=(X_test, y_test), epochs=10, batch_size=100)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model_larger.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "print(\"Larger CNN Error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFU5-W445_hq"
      },
      "source": [
        "# Choosing final Model\n",
        "From above all the cases as we can see the last model is having the best accuracy. \n",
        "Training convolutional neural networks is a random process. This makes experiments difficult because each time you run the same experiment, you get different results. Therefore, you must run your experiments dozens of times and take an average. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RyL-1UV5_hr"
      },
      "source": [
        "import pandas as pd\n",
        "X_test = pd.read_csv('/kaggle/input/digit-recognizer/test.csv').values.astype('float32')\n",
        "X_test = X_test.reshape(-1, 28, 28, 1)\n",
        "X_test = X_test.astype('float32')/255\n",
        "testY = model_larger.predict_classes(X_test, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXDXUFMH5_hu"
      },
      "source": [
        "sub = pd.read_csv('/kaggle/input/digit-recognizer/sample_submission.csv')\n",
        "sub['Label'] = testY\n",
        "sub.to_csv('submission.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dejhxE8k5_hx"
      },
      "source": [
        "to know how to make neural network without using any deep learning tools(Like: Tensorflow, Pytorch,etc) got through my another notebook: [Create Neural Network From Scratch](https://www.kaggle.com/soham1024/create-neural-network-from-scratch)     \n",
        "\n",
        "### how inside the Neural Network work is done: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "id": "06A1rYEo5_hy"
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('3JQ3hYko51Y', width=800, height=450)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvZewvkf5_h2"
      },
      "source": [
        "reference: \n",
        "* https://keras.io/\n",
        "* https://www.kaggle.com/cdeotte/how-to-choose-cnn-architecture-mnist\n",
        "* https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6"
      ]
    }
  ]
}